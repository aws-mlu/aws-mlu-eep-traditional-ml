{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffb6761",
   "metadata": {},
   "source": [
    "<center><img src=\"images/logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# Application of Deep Learning to Text and Image Data\n",
    "## Module 3, Lab 5: Using the ConvNeXt Model\n",
    "\n",
    "This notebook explains how to import the ConvNeXt model and use it to make predictions on a small selection of example images.\n",
    "\n",
    "You will learn how to do the following:\n",
    "\n",
    "- Download the ConvNext pre-trained model\n",
    "- Pre-process images\n",
    "- Use ConvNext to perform inference on images\n",
    "\n",
    "----\n",
    "\n",
    "You will be presented with a challenge at the end of this lab:\n",
    "\n",
    "| <img style=\"float: center;\" src=\"images/challenge.png\" alt=\"Challenge\" width=\"125\"/> |\n",
    "| --- |\n",
    "| <p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3d6a0-e4df-4ec0-a1b7-b29bfa7e9196",
   "metadata": {},
   "source": [
    "---\n",
    "## Index\n",
    "\n",
    "- [Downloading a pretrained model](#Downloading-a-pretrained-model)\n",
    "- [Preprocessing an image](#Preprocessing-an-image)\n",
    "- [Inference using ConvNeXt](#Inference-using-ConvNeXt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9e56d",
   "metadata": {},
   "source": [
    "---\n",
    "## Downloading a pretrained model\n",
    "\n",
    "With Torchvision, you can use a ConvNeXt model that is trained on the ImageNet dataset as the base model. You will import the `convnext_tiny` model with the `IMAGENET1K_V1` weights. This model is the smallest size ConvNeXt model from its original implementation. Its weights are associated with the ImageNet dataset with 1K classes.\n",
    "\n",
    "For more information about the `convnext_tiny` model, see [convnext_tiny](https://pytorch.org/vision/main/models/generated/torchvision.models.convnext_tiny.html) in the PyTorch documentation.\n",
    "\n",
    "For information about other pretrained models, see [Models and Pre-trained Weights](https://pytorch.org/vision/stable/models.html) in the PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d73eddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be182bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torchvision\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import convnext_tiny\n",
    "\n",
    "# Use GPU resource if available; otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = convnext_tiny(weights=\"IMAGENET1K_V1\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536bed1",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing an image\n",
    "\n",
    "After importing the dataset, read a sample image and preprocess it with preset data transforms by using `transform_eval`.\n",
    "\n",
    "The sample image is an American kitchen scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffce50-5abb-4e8f-9c91-8a8298ed3b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_raw = Image.open('../data/minc-2500/test/food/food_001611.jpg').convert('RGB')\n",
    "plt.imshow(img_raw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7299d9f-8679-47dd-b3de-e4d3a7f522cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img_tr = transform(img_raw)\n",
    "\n",
    "# calculate mean and std\n",
    "mean, std = img_tr.mean([1,2]), img_tr.std([1,2])\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd697ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)\n",
    "])\n",
    "\n",
    "img = transform_eval(img_raw).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec3ea9",
   "metadata": {},
   "source": [
    "---\n",
    "## Inference using ConvNeXt\n",
    "\n",
    "Now that the sample image is imported and preprocessed, you will use the pretrained ConvNeXt model to predict the class for this image. You will use the `net` object that was defined in a previous cell to evaluate and generate predictions. Output will be saved to the `pred` variable, which will consist of a list of ndarrays, where each ndarray is of length 1,000.\n",
    "\n",
    "Each number of this 1,000-length ndarray can have `softmax` applied to represent the prediction confidence toward each of the [subject classes in ImageNet](https://image-net.org/challenges/LSVRC/index.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d511d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "pred = net(img.unsqueeze(0))\n",
    "\n",
    "print(\"Number of classes as output\")\n",
    "print(len(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73118aba",
   "metadata": {},
   "source": [
    "PyTorch doesn't offer a built-in function to access all the existing classes of ImageNet. To see the existing classes, you need to load an external JSON list that has a mapping for the ImageNet classes to the indices. The dataset has 1,000 classes (therefore, the model weights are called `IMAGENET1K_V1`). Import and look at the first 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df3dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../data/imagenet_idx_to_class.json\", \"r\") as f:\n",
    "    classes = json.load(f)\n",
    "\n",
    "print(classes[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d58302",
   "metadata": {},
   "source": [
    "The first 5 classes don't have any relation to the sample image of a kitchen. Therefore, you will need to compare the list of model predictions (the `pred` variable) to the list of classes that you just imported. Let's see what the model thinks of the input test image.\n",
    "\n",
    "The model made 1,000 predictions, but the further down the list the prediction is, the lower the probability of it being a correct prediction. For this reason, you only need to print the top 5 results (classes) that the model returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81458dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topK = 5\n",
    "conf, ind = torch.topk(pred.squeeze(0), k=topK)\n",
    "ind = ind.squeeze(0).cpu().numpy()\n",
    "print('The input picture is classified to be')\n",
    "for i in range(topK):\n",
    "    print('\\t[%s], with probability %.3f.'%\n",
    "          (classes[ind[i]], torch.softmax(pred.squeeze(0), dim=0)[ind[i]].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2ac99",
   "metadata": {},
   "source": [
    "The results are related to to kitchen items. Remember that the input was an image of a kitchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02393a16",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Use the following code cell to make a prediction with another image.</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">Use the image <code>data/minc-2500/test/water/water_001765.jpg</code> and predict its class.</p><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c0a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine if a GPU is availible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up the convnext_tiny model\n",
    "net = convnext_tiny(weights=\"IMAGENET1K_V1\").to(device)\n",
    "\n",
    "############### CODE HERE ###############s\n",
    "\n",
    "img_raw = Image.open('../data/minc-2500/test/water/water_001765.jpg').convert('RGB')\n",
    "plt.imshow(img_raw)\n",
    "plt.show()\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img = transform_eval(img_raw).to(device)\n",
    "\n",
    "pred = net(img.unsqueeze(0))\n",
    "\n",
    "with open(\"../data/imagenet_idx_to_class.json\", \"r\") as f:\n",
    "    classes = json.load(f)\n",
    "\n",
    "topK = 5\n",
    "conf, ind = torch.topk(pred.squeeze(0), k=topK)\n",
    "ind = ind.squeeze(0).cpu().numpy()\n",
    "print('The input picture is classified to be')\n",
    "for i in range(topK):\n",
    "    print('\\t[%s], with probability %.3f.'%\n",
    "          (classes[ind[i]], torch.softmax(pred.squeeze(0), dim=0)[ind[i]].item()))\n",
    "\n",
    "############## END OF CODE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235c143-2d18-4116-abb4-a0a01bcdf5de",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this Lab, you saw how to import and use the ConvNeXt model to make predictions without training your own model. This is an easy and cost-effective way to get started with making predictions. The biggest problem is that the model probably won't fit well to your dataset, which means that you need to fine-tune the model to make good predictions.\n",
    "\n",
    "--- \n",
    "## Next lab\n",
    "In the next lab, you will learn how to fine-tune the ConvNeXt model to make better predictions that align with your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
