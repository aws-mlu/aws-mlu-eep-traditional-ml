{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc91120c",
   "metadata": {},
   "source": [
    "<center><img src=\"../common/images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"600\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# Application of Deep Learning to Text and Image Data\n",
    "## Module 2, Lab 5: Finetuning BERT\n",
    "\n",
    "\n",
    "BERT stands for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers. To learn how BERT works, let's fine-tune the __BERT__ model to classify product reviews. You will use a new library called __transformers__ to download a pre-trained BERT model. \n",
    "\n",
    "You will learn:\n",
    "\n",
    "- How to load and format the dataset\n",
    "- How to load the pre-trained model\n",
    "- How to train and test the model\n",
    "\n",
    "__BERT and its variants use more resources than the other models you have used so far. This may cause your instance to run out of memory. If that happens:__\n",
    "\n",
    "- Restart the kernel (Kernel->Restart from the top menu)\n",
    "- Reduce the batch size \n",
    "- Then re-run the code\n",
    "\n",
    "\n",
    "\n",
    "__Note__: In this walkthrough, you will use a light version of the original BERT implementation called __\"DistilBert\"__. You can checkout [the paper](https://arxiv.org/pdf/1910.01108.pdf) about it for more details. \n",
    "\n",
    "---\n",
    "This lab uses a dataset derived from a small sample of Amazon product reviews. \n",
    "\n",
    "__Review dataset schema:__\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log\\_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"../common/images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"../common/images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc56e8-353d-4511-a225-eb2521cf946a",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "1. [Reading and formatting the dataset](#Reading-and-formatting-the-dataset)\n",
    "1. [Loading the pre-trained model](#Loading-the-pre-trained-model)\n",
    "1. [Training and testing the model](#Training-and-testing-the-model)\n",
    "1. [Getting predictions on the test data](#Getting-predictions-on-the-test-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d3332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3c795",
   "metadata": {},
   "source": [
    "## Reading and formatting the dataset\n",
    "\n",
    "First, you need to read in the product review dataset and prepare it for the BERT model. To keep the training time down, you will only use the first 2000 data points from the dataset. If you want to improve your model after you understand how to train, you can use more data to train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782fdf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import system library and append path\n",
    "sys.path.insert(1, '..')\n",
    "\n",
    "# Setting tokenizer parallelism to false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Import utility functions that provide answers to challenges\n",
    "from MLUDTI_EN_M2_Lab5_quiz_questions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926dfe2d",
   "metadata": {},
   "source": [
    "Read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83f37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/NLP-REVIEW-DATA-CLASSIFICATION-TRAINING.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2a4ec",
   "metadata": {},
   "source": [
    "Print the dataset information to see the field types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc624ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a4c80",
   "metadata": {},
   "source": [
    "You do not need any of the rows that do not have __reviewText__, so drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f993d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"reviewText\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efeb89-d57e-41b5-afa9-0d4fea1bf745",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../common/images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the question below to test your understanding of epochs and learning rate.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafb228-0fb2-4d35-8b95-2bed396a0714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2e21c",
   "metadata": {},
   "source": [
    "BERT requires a lot of compute power for large datasets. To reduce the amount of time it takes to train the model, you will only use the first 2,000 data points for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b01ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.head(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2d8b9",
   "metadata": {},
   "source": [
    "Now split the dataset into training and validation data sets, keeping 10% of the data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdd5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This separates 10% of the entire dataset into validation dataset.\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"reviewText\"].tolist(),\n",
    "    df[\"isPositive\"].tolist(),\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    "    stratify = df[\"isPositive\"].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f7323",
   "metadata": {},
   "source": [
    "You need to tokenize the data. To do this, use a special tokenizer built for the DistilBERT model to tokenize the training and validation texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ff8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "val_encodings = tokenizer(val_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92881b",
   "metadata": {},
   "source": [
    "Create a new `ReviewDataset` class to use with the BERT model. Later, you use the training and validation encoding-label pairs with this new class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a8e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "val_dataset = ReviewDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bbfae6",
   "metadata": {},
   "source": [
    "## Loading the pre-trained model\n",
    "\n",
    "Now, you need to load the model. When you do this, several warnings will print that are related to the last classification layer of BERT where you are using a randomly initialized layer. You can ignore the warnings as they are not relevant to the type of training you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e0ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                            num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa228cc",
   "metadata": {},
   "source": [
    "The last step is to freeze all weights until the very last classification layer in the BERT model. This helps accelerate the training process. Training the weights of the whole network (66 million weights) takes a long time. Additionally, 2000 data points would not be enough for that task. Instead, the code below freezes all the weights until the last classification layer. This means only a small portion of the weights gets updated (rest stays the same). This is a common practice with large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5468e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the encoder weights until the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015dfe3",
   "metadata": {},
   "source": [
    "## Training and testing the model\n",
    "\n",
    "Now that your data is ready and you have configured your model, its time to start the fine-tuning process. This code will take __a long time__ (30+ minutes) to complete with large datasets, that is why you are running it on a subset of the full review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a0f41",
   "metadata": {},
   "source": [
    "First, define the accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a035a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, label):\n",
    "    \"\"\"Calculate the accuracy of the trained network. \n",
    "    output: (batch_size, num_output) float32 tensor\n",
    "    label: (batch_size, ) int32 tensor \"\"\"\n",
    "    \n",
    "    return (output.argmax(axis=1) == label.float()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca5e8a",
   "metadata": {},
   "source": [
    "Now you need to create the tranining and validation loop. This loop will be similar to the previous train/validation loops, however there are a few extra parameters needed due to the transformer architecture. \n",
    "\n",
    "You need to use the `attention_mask` and get the loss from the output of the model with `loss = output[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e98aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,\n",
    "                          batch_size=16, drop_last=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8,\n",
    "                               drop_last=True)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, val_loss, train_acc, valid_acc = 0., 0., 0., 0.\n",
    "    \n",
    "    start = time.time()\n",
    "    # Training loop starts\n",
    "    model.train() # put the model in training mode\n",
    "    for batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Put data, label and attention mask to the correct device\n",
    "        data = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Make forward pass\n",
    "        output = model(data, attention_mask=attention_mask, labels=label)\n",
    "        \n",
    "        # Calculate the loss (this comes from the output)\n",
    "        loss = output[0]\n",
    "        # Make backwards pass (calculate gradients)\n",
    "        loss.backward()\n",
    "        # Accumulate training accuracy and loss\n",
    "        train_acc += calculate_accuracy(output.logits, label).item()\n",
    "        train_loss += loss.item()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop:\n",
    "    # This loop tests the trained network on validation dataset\n",
    "    # No weight updates here\n",
    "    # torch.no_grad() reduces memory usage when not training the network\n",
    "    model.eval() # Activate evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            data = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch[\"labels\"].to(device)\n",
    "            # Make forward pass with the trained model so far\n",
    "            output = model(data, attention_mask=attention_mask, labels=label)\n",
    "            # Accumulate validation accuracy and loss\n",
    "            valid_acc += calculate_accuracy(output.logits, label).item()\n",
    "            val_loss += output[0].item()\n",
    "        \n",
    "    # Take averages\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "    val_loss /= len(validation_loader)\n",
    "    valid_acc /= len(validation_loader)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Epoch %d: train loss %.3f, train acc %.3f, val loss %.3f, val acc %.3f, seconds % .3f \" % (\n",
    "        epoch+1, train_loss, train_acc, val_loss, valid_acc, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7ce78",
   "metadata": {},
   "source": [
    "### Looking at what's going on\n",
    "\n",
    "The fine-tuned BERT model is able to correctly classify the sentiment of the most of the records in the validation set. Let's observe in more detail how the sentences are tokenized and encoded. You can do this by picking one sentence as example to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195dd23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = val_texts[19]\n",
    "print(f\"Sentence: {st}\")\n",
    "tok = tokenizer(st, truncation=True, padding=True)\n",
    "print(f\"Encoded Sentence: {tok['input_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5359be5",
   "metadata": {},
   "source": [
    "Print the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78bc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The mapped vocabulary is stored in tokenizer.vocab\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738febb",
   "metadata": {},
   "source": [
    "Use the encoded sentence with the tokenizer to recover the original sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ae225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Methods convert_ids_to_tokens and convert_tokens_to_ids allow to see how sentences are tokenized\n",
    "print(tokenizer.convert_ids_to_tokens(tok[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89346761",
   "metadata": {},
   "source": [
    "## Getting predictions on the test data\n",
    "\n",
    "After the model is trained, you can focus on getting test data to make predictions with. Do this by:\n",
    "- Reading and format the test dataset\n",
    "- Passing the test data to your trained model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36b78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the test data (It doesn't have the isPositive label)\n",
    "df_test = pd.read_csv(\"../data/NLP-REVIEW-DATA-CLASSIFICATION-TEST.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6477c10",
   "metadata": {},
   "source": [
    "Just as before, drop the rows that don't have the __reviewText__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb479bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.dropna(subset=[\"reviewText\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588e159",
   "metadata": {},
   "source": [
    "Making predictions will also take a long time with this model. To get results quickly, start by only making predictions with 15 datapoints from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35daf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_texts = df_test[\"reviewText\"].tolist()[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e9cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts,\n",
    "                           truncation=True,\n",
    "                           padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640fe83",
   "metadata": {},
   "source": [
    "Create labels for the test dataset to pass zeros using `[0]*len(test_texts)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8ba88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = ReviewDataset(test_encodings, [0]*len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb780",
   "metadata": {},
   "source": [
    "Then, create a dataloader for the test set and record the corresponding predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012e571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        data = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "        output = model(data, attention_mask=attention_mask, labels=label)\n",
    "        predictions = torch.argmax(output.logits, dim=-1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202c1f0",
   "metadata": {},
   "source": [
    "Finally, pick an example sentence and examine the prediction. Does the prediction look correct? \n",
    "\n",
    "Remember \n",
    "\n",
    "- 1->positive class \n",
    "- 0->negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674a041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 13\n",
    "print(f'Text: {test_texts[k]}')\n",
    "print(f'Prediction: {test_predictions[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c89a3",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center; margin:auto;\"><img src=\"../common/images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">You trained the model for 10 epochs. Would you get better results from the validation dataset if the model trained longer?</p> <br>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Make a note of your last <code> Val_loss </code> result.</p> \n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Then, in the <a href=\"#Training-and-testing-the-model\">Training and testing the model</a> section, change the <code> num_epochs </code> parameter to <code>20</code>.</p>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Finally, re-run the code blocks to load the pre-trained model, and train your model.</p>\n",
    "    <p style=\"margin: auto; text-align: center; margin: auto;\">Did <code>Val_loss</code> improve?</p>\n",
    "    </ol>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ece07",
   "metadata": {},
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "In this lab you learned how to import a pre-trained Transformer model and fine-tune it for a specific task. Although you used a lighter version of the BERT model, these types of models tend to use large amounts of compute power. For that reason, you only worked with the first 2000 datapoints of the dataset. To see more general results, you need to spend more time training while using the whole dataset. \n",
    "\n",
    "## Next Lab: Reading and plotting images\n",
    "In the next lab you will learn how to read images and plot them as you start to learn about computer vision problems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
