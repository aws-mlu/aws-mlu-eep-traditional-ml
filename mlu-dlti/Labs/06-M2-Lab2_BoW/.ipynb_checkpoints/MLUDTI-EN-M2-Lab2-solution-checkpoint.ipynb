{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../common/images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"600\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# Application of Deep Learning to Text and Image Data\n",
    "## Module 2, Lab 2: Using the BoW Method\n",
    "\n",
    "This notebook will help you understand how to further process text data through *vectorization*. You will explore the bag-of-words (BoW) method to convert text data into numerical values, which will be used later for predictions with ML algorithms.\n",
    "\n",
    "To convert text data to vectors of numbers, a vocabulary of known words (tokens) is extracted from the text. The occurrence of words is scored, and the resulting numerical values are saved in vocabulary-long vectors. A few versions of BoW exist with different word-scoring methods.\n",
    "\n",
    "You will learn the following:\n",
    "- How to use sklearn to process text in several ways\n",
    "- When to use each method\n",
    "- How to calculate BoW numerical values\n",
    "- How to use binary classification, word counts, term frequency (TF), and term frequency-inverse document frequency (TF-IDF)\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"../common/images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"../common/images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Index\n",
    "\n",
    "* [Binary classification](#Binary-classification)\n",
    "* [Word counts](#Word-counts)\n",
    "* [Term frequency](#Term-frequency)\n",
    "* [Inverse document frequency](#Inverse-document-frequency)\n",
    "* [Term frequency-inverse document frequency](#Term-frequency-inverse-document-frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Binary classification\n",
    "\n",
    "The first BoW method that you will use is *binary classification*. This method records whether a word is in a given sentence. You will also experiment with sklearn's vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This document is the first document\",\n",
    "    \"This document is the second document\",\n",
    "    \"and this is the third one\",\n",
    "]\n",
    "\n",
    "# Initialize the count vectorizer with the parameter binary=True\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# The fit_transform() function fits the text data and gets the binary BoW vectors\n",
    "x = binary_vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the vocabulary size grows, the BoW vectors get large. They usually have many zeros and few nonzero values. Sklearn stores these vectors in a compressed form. If you want to use them as NumPy arrays, call the `toarray()` function.\n",
    "\n",
    "The following are the binary BoW features. Each row in the printed array corresponds to a single document binary encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this array represents, check the vocabulary by using the `vocabulary` attribute. This returns a dictionary with each word as key and index as value. Notice that the indices are assigned in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_feature_names_out()` function displays similar information. The position of the terms in the output corresponds to the column position of the elements in the BoW matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(binary_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does this data mean?\n",
    "\n",
    "First, you created a list of three sentences. Each sentence contains six words.\n",
    "\n",
    "Next, you created a vectorizer. This vectorizer collected all the words, ordered them alphabetically, and removed any duplicates.\n",
    "\n",
    "You then converted the sentences to an array. The array has nine columns for each row. The nine columns correspond to the nine unique words from the sentences.\n",
    "\n",
    "When you add column headers and identify the rows as sentences, as in the following table, you can see that the array tells you whether a word is included in the sentence. However, the array doesn't tell you how many times the word is used or where it appears in the sentence. \n",
    "\n",
    "| Number | Sentence | and | document | first | is | one | second | the | third | this |\n",
    "| - | - | :-: | :------: | :---: | :-: | :-: | :----: | :-: | :---: | :--: |\n",
    "| 1 | This document is the first document  | no | yes | yes | yes | no | no | yes | no | yes|\n",
    "| 2 | This document is the second document | no | yes | no | yes | no | yes | yes |no | yes |\n",
    "| 3 | and this is the third one            | yes | no | no | yes | yes | no | yes | yes | yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can compute how many sentences each word from the vocabulary appears in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../common/images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">To show each word and its frequency (the number of times that it was used in all of the sentences), run the following cell.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "sum_words = x.sum(axis=0)\n",
    "words_freq = [\n",
    "    (word, sum_words[0, idx])\n",
    "    for (idx, word) in enumerate(binary_vectorizer.get_feature_names_out())\n",
    "]\n",
    "words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `binary_vectorizer` function to automatically create a table that shows the BoW vectors that are associated to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    x.toarray(), columns=binary_vectorizer.get_feature_names_out(), index=sentences\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you calculate BoW vectors for a new sentence?\n",
    "\n",
    "You can use the `transform()` function. When you look at the results, notice that this doesn't change the vocabulary. New words are simply skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_sentence = [\"This is the new sentence\"]\n",
    "\n",
    "new_vectors = binary_vectorizer.transform(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "        <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../common/images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">To generate whether each word in the corpus appears for each sentence, run the following cell.</p>\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    new_vectors.toarray(),\n",
    "    columns=binary_vectorizer.get_feature_names_out(),\n",
    "    index=new_sentence,\n",
    ")\n",
    "pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that __new__ and __sentence__ aren't listed in the vocabulary, but the other words are listed correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Word counts\n",
    "\n",
    "You can calculate word counts by using the same `CountVectorizer()` function _without_ the `binary` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This document is the first document\",\n",
    "    \"This document is the second document\",\n",
    "    \"and this is the third one\",\n",
    "]\n",
    "\n",
    "# Initialize the count vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "xc = count_vectorizer.fit_transform(sentences)\n",
    "\n",
    "xc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    xc.toarray(), columns=binary_vectorizer.get_feature_names_out(), index=sentences\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\"> \n",
    "       <h3><i>Try it yourself!</i></h3>\n",
    "    <p style=\"text-align:center; margin:auto;\"><img src=\"../common/images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">In the following code cell, use the <code>transform()</code> function to calculate BoW vectors for a new piece of text.</p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Note:</b> A similar example of how to use the <code>.transform()</code> function is available in the Binary Classification section of this notebook.</p>\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_sentence = [\"This is the new sentence\"]\n",
    "\n",
    "############### CODE HERE ###############\n",
    "\n",
    "new_vectors = count_vectorizer.transform(new_sentence)\n",
    "new_vectors.toarray()\n",
    "\n",
    "############## END OF CODE ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    new_vectors.toarray(),\n",
    "    columns=binary_vectorizer.get_feature_names_out(),\n",
    "    index=new_sentence,\n",
    ")\n",
    "pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Term frequency\n",
    "\n",
    "Term frequency (TF) vectors show the importance of words in a document. These vectors are computed with the following formula:\n",
    "\n",
    "$$tf(term, doc) = \\frac{\\text{Number of times that the term occurs in the doc}}{\\text{Total number of terms in the doc}}$$\n",
    "\n",
    "To calculate TF, you will use sklearn's `TfidfVectorizer` function with the parameter `use_idf=False`, which *automatically normalizes* the TF vectors by their Euclidean ($L_2$) norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "x = tf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "np.round(x.toarray(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_sentence = [\"This is the new sentence\"]\n",
    "new_vectors = tf_vectorizer.transform(new_sentence)\n",
    "np.round(new_vectors.toarray(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "      <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../common/images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">To generate the TF vector for each sentence, run the following cell.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    np.round(x.toarray(), 2), columns=tf_vectorizer.get_feature_names_out(), index=sentences\n",
    ")\n",
    "df2 = pd.DataFrame(\n",
    "    np.round(new_vectors.toarray(), 2),\n",
    "    columns=tf_vectorizer.get_feature_names_out(),\n",
    "    index=new_sentence,\n",
    ")\n",
    "pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inverse document frequency\n",
    "\n",
    "Inverse Document Frequency (IDF) is a weight indicating how commonly a word is used. The more frequent its usage across documents, the lower its score. The lower the score, the less important the word becomes.\n",
    "\n",
    "It is computed with the following formula: \n",
    "\n",
    "$$idf(term) = \\ln \\Big( \\frac{n_{documents}}{n_{documents\\,containing\\,the\\,term}}\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Term frequency-inverse document frequency\n",
    "\n",
    "Term frequency-inverse document frequency (TF-IDF) is computed by the following formula:\n",
    "\n",
    "$$tf-idf(term,doc) = tf(term,doc)*idf(term)$$\n",
    "</br>\n",
    "\n",
    "Using sklearn, vectors are computed using the `TfidfVectorizer()` function with the parameter `use_idf=True`.\n",
    "\n",
    "__Note:__ You don't need to include the parameter because it is `True` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "sentences = [\n",
    "    \"This document is the first document\",\n",
    "    \"This document is the second document\",\n",
    "    \"and this is the third one\",\n",
    "]\n",
    "\n",
    "xf = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "np.round(xf.toarray(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_sentence = [\"This is the new sentence\"]\n",
    "new_vectors = tfidf_vectorizer.transform(new_sentence)\n",
    "np.round(new_vectors.toarray(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    np.round(xf.toarray(), 2),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=sentences,\n",
    ")\n",
    "df2 = pd.DataFrame(\n",
    "    np.round(new_vectors.toarray(), 2),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=new_sentence,\n",
    ")\n",
    "pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ In addition to automatically normalizing the TF vectors by their Euclidean ($L_2$) norm, sklearn also uses a *smoothed version of idf* and computes the following: \n",
    "\n",
    "$$idf(term) = \\ln \\Big( \\frac{n_{documents} +1}{n_{documents\\,containing\\,the\\,term}+1}\\Big) + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.round(tfidf_vectorizer.idf_, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the IDF is larger for the less common terms.\n",
    "\n",
    "Now you can generate the IDF DataFrame and TF DataFrame, and then concatenate them as one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [[str(a) for a in np.round(tfidf_vectorizer.idf_, 2)]],\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=[\"IDF\"],\n",
    ")\n",
    "df2 = pd.DataFrame(\n",
    "    [[str(w[1]) for w in words_freq]],\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=[\"TF\"],\n",
    ")\n",
    "pd.concat([df2, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows that when the TF is large, the IDF is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you observed how the BoW method converts text data into numerical values.\n",
    "\n",
    "--- \n",
    "## Next lab\n",
    "In the next lab, you will explore advanced word embeddings and the relationships between words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
